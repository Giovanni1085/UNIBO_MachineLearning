{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Processing\n",
    "\n",
    "In this notebook, we explore text classification and entity recognition. \n",
    "\n",
    "We start by introducing the main architecture currently used to process texts: the Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer\n",
    "\n",
    "Published in the paper [Attention is All you Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017), the Transformer and its variants have become the main neural network architecture across data modalities. The Transformer is [natively implemented in PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) as a network model.\n",
    "\n",
    "In our introduction to this architecture, we borrow from the following sources:\n",
    "1. [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/).\n",
    "2. [A Very Gentle Introduction to Large Language Models without the Hype](https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e).\n",
    "3. [Tutorial 6: Transformers and Multi-Head Attention](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html).\n",
    "4. [The Illustrated GPT2](https://jalammar.github.io/illustrated-gpt2/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "\n",
    "**Language models** allow us to represent textual inputs numerically, all the while storing information about all layers of the linguistic stack, from syntax to semantics, and more.\n",
    "\n",
    "Every language model starts with the need to represent text numerically. This step is often called **tokenization**: the slicing up of a text into units that are assigned a vector representation, or **embedding**, before being further processed by the language model. The most widely used tokenizers nowadays work at the subword level, i.e., short sequences of characters. Many different alternatives are possible.\n",
    "\n",
    "Once a text has been tokenized, and in order to train a language model, an approach is taken which is called **self-supervision**: large quantities of human-generated texts are used with words masked at random or sequentially, and the model is trained on the task of predicting the missing words. This simple approach turns out to be extremely powerful in practice. See [2] for more.\n",
    "\n",
    "Random masking:\n",
    "\n",
    "<img src=\"figures/mask1.png\" width=\"400px\" heigth=\"400px\">\n",
    "\n",
    "Sequential masking, or next word prediction:\n",
    "\n",
    "<img src=\"figures/mask2.png\" width=\"400px\" heigth=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"GPT stands for **Generative Pre-trained Transformer**. Let’s break this down:\n",
    "\n",
    "* Generative. The model is capable of generating continuations to the provided input. That is, given some text, the model tries to guess which words come next.\n",
    "* Pre-trained. The model is trained on a very large corpus of general text and is meant to be trained once and used for a lot of different things without needing to be re-trained from scratch.\n",
    "\n",
    "A **transformer** is a particular type of deep learning model that transforms the encoding in a particular way that makes it easier to guess the blanked out word. At the heart of a transformer is the classical **encoder-decoder network**. The encoder does a very standard encoding process. But then it adds something else called **self-attention**.\n",
    "\n",
    "A transformer learns which words in an input sequence are related and then creates a new encoding for each position in the input sequence that is a merger of all the related words.\" [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/transformer_architecture.svg\" width=\"400px\" heigth=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder Architectures\n",
    "\n",
    "<img src=\"figures/encoderdecoder1.png\" width=\"400px\" heigth=\"400px\">\n",
    "\n",
    "The basic idea of Encoder-Decoder architectures originates by the need to deal with so called *\"sequence to sequence tasks\"*, e.g., translation. The input, for example your text in French, is first *encoded*, and then the output, for example a translation in English, is generated by the *decoder* on the basis of the encoded input. It turns out that these architectures are also generally useful when you want to encode a query, and generate an answer, like ChatGPT does.\n",
    "\n",
    "<img src=\"figures/encoderdecoder3.png\" width=\"500px\" heigth=\"300px\">\n",
    "\n",
    "In the encoder and the decoder, we can use a variety of modules. \n",
    "\n",
    "With transformers, feed forward layers are used in conjunction with self-attention layers. When the encoded input is passed to the decoder, another form of attention is applied: cross-attention:\n",
    "\n",
    "<img src=\"figures/encoderdecoder2.png\" width=\"500px\" heigth=\"300px\">\n",
    "\n",
    "In cross-attention, the queries (*Q*) come from the decoder hidden states, while the keys (*K*) and values (*V*) come from the encoder hidden states. We will see next what this means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "<img src=\"figures/attention1.png\" width=\"500px\" heigth=\"400px\">\n",
    "\n",
    "The key idea of attention is to allow the model to look at, or attend, at any relevant part of the input when making a prediction, for example when predicting a masked word, or translating a single word into another language.\n",
    "\n",
    "<img src=\"figures/attention2.png\" width=\"400px\" heigth=\"400px\">\n",
    "\n",
    "Attention units can be visualized and show how a model might consider different parts of the input at a given step, with varied intensity based on how relevant each part of the input is to the prediction at this step.\n",
    "\n",
    "#### Self-Attention\n",
    "\n",
    "Attention layers work by introducing three sets of parameters: *queries Q, keys K, and values V*.\n",
    "\n",
    "Following [1], we have that at each calculation of self-attention:\n",
    "\n",
    "1. The first step is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a query vector, a key vector, and a value vector. These vectors are created by multiplying the embedding by the three matrices Q, K, V, that are trained during the training process.\n",
    "2. The second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.\n",
    "3. The third step applies a normalization and the softmax, to get probability scores for each input word against the current word.\n",
    "4. The next step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and reduce the weight of irrelevant words.\n",
    "5. The fifth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).\n",
    "\n",
    "<img src=\"figures/selfattention.png\" width=\"500px\" heigth=\"800px\">\n",
    "\n",
    "These operations are easily implemented as matrix multiplications. \n",
    "\n",
    "Note again that **cross-attention** works in a similar way, just the queries come from the encoder, whilst the keys and values from the decoder. Lastly, note that the transformer used **multi-head attention**: several attentio modules or heads are stacked up in each layer, the model training determine how they are actually used. This gives the model much more capacity.\n",
    "\n",
    "*See 6.2. How Does Self-Attention Work? [2] and [1] here for an in-depth discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the transformer is a general purpose architecture. It has been used in encoder-only architectures (e.g., BERT: Bidirectional Encoder Representations from Transformers), in vision architectures (e.g., ViT: Vision Transformer), and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that further tools used to improve GPTs after language model training:\n",
    "* **Instruction tuning**: basically, provide correct answers to the question you asked (i.e., supervised learning).\n",
    "* **Reinforcement learning from human feedback**: provide signal on whether the answer is good or bad (e.g., thumbs up or down). This is used as signal by the LLM to improve itself.\n",
    "\n",
    "See [4] for more details on these aspects.\n",
    "\n",
    "*This is a very quick introduction to the transformer, aiming only at providing intuition. In most cases, as we will see below, transformers are used as pre-trained models, that we then fine-tune or otherwise adjust to our specific tasks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "sns.set_theme('notebook', style='whitegrid')\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set device (GPU or CPU)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 2.2.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42) # Setting the seed\n",
    "\n",
    "print(\"Using torch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification\n",
    "\n",
    "In this exercise, we are given a dataset of digitized books from the British Library. These books belong to different genres (e.g., poetry or prose). We are interested in training a classifier to distinguish between such genres. [Read more about this dataset here](https://github.com/mromanello/ADA-DHOxSS/tree/master/data#british-library-19th-century-books)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = pd.read_csv('data/bl_books/sample_tidy/df_book.csv')\n",
    "df_texts = pd.read_csv('data/bl_books/sample_tidy/df_book_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datefield</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>edition</th>\n",
       "      <th>place</th>\n",
       "      <th>issuance</th>\n",
       "      <th>first_pdf</th>\n",
       "      <th>number_volumes</th>\n",
       "      <th>identifier</th>\n",
       "      <th>fulltext_filename</th>\n",
       "      <th>type</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1841.0</td>\n",
       "      <td>Privately printed</td>\n",
       "      <td>The Poetical Aviary, with a bird's-eye view of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Calcutta</td>\n",
       "      <td>monographic</td>\n",
       "      <td>lsidyv35c55757</td>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>000000196_01_text.json</td>\n",
       "      <td>poet</td>\n",
       "      <td>Poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1888.0</td>\n",
       "      <td>Rivingtons</td>\n",
       "      <td>A History of Greece. Part I. From the earliest...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>monographic</td>\n",
       "      <td>lsidyv376da437</td>\n",
       "      <td>1</td>\n",
       "      <td>4047</td>\n",
       "      <td>000004047_01_text.json</td>\n",
       "      <td>story</td>\n",
       "      <td>Prose</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   datefield          publisher  \\\n",
       "0     1841.0  Privately printed   \n",
       "1     1888.0         Rivingtons   \n",
       "\n",
       "                                               title edition     place  \\\n",
       "0  The Poetical Aviary, with a bird's-eye view of...     NaN  Calcutta   \n",
       "1  A History of Greece. Part I. From the earliest...     NaN    London   \n",
       "\n",
       "      issuance       first_pdf  number_volumes  identifier  \\\n",
       "0  monographic  lsidyv35c55757               1         196   \n",
       "1  monographic  lsidyv376da437               1        4047   \n",
       "\n",
       "        fulltext_filename   type   genre  \n",
       "0  000000196_01_text.json   poet  Poetry  \n",
       "1  000004047_01_text.json  story   Prose  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "Music     119\n",
       "Poetry    114\n",
       "Drama     111\n",
       "Prose     108\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books.genre.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_books.merge(df_texts, on='fulltext_filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datefield</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>edition</th>\n",
       "      <th>place</th>\n",
       "      <th>issuance</th>\n",
       "      <th>first_pdf</th>\n",
       "      <th>number_volumes</th>\n",
       "      <th>identifier</th>\n",
       "      <th>fulltext_filename</th>\n",
       "      <th>type</th>\n",
       "      <th>genre</th>\n",
       "      <th>fulltext</th>\n",
       "      <th>book_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1841.0</td>\n",
       "      <td>Privately printed</td>\n",
       "      <td>The Poetical Aviary, with a bird's-eye view of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Calcutta</td>\n",
       "      <td>monographic</td>\n",
       "      <td>lsidyv35c55757</td>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>000000196_01_text.json</td>\n",
       "      <td>poet</td>\n",
       "      <td>Poetry</td>\n",
       "      <td>THE POETICAL AVIARY, WITH A B I R D'S-E ...</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1888.0</td>\n",
       "      <td>Rivingtons</td>\n",
       "      <td>A History of Greece. Part I. From the earliest...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>monographic</td>\n",
       "      <td>lsidyv376da437</td>\n",
       "      <td>1</td>\n",
       "      <td>4047</td>\n",
       "      <td>000004047_01_text.json</td>\n",
       "      <td>story</td>\n",
       "      <td>Prose</td>\n",
       "      <td>HISTORY OF GREECE ABBOTT  A HISTORY OF G...</td>\n",
       "      <td>4047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   datefield          publisher  \\\n",
       "0     1841.0  Privately printed   \n",
       "1     1888.0         Rivingtons   \n",
       "\n",
       "                                               title edition     place  \\\n",
       "0  The Poetical Aviary, with a bird's-eye view of...     NaN  Calcutta   \n",
       "1  A History of Greece. Part I. From the earliest...     NaN    London   \n",
       "\n",
       "      issuance       first_pdf  number_volumes  identifier  \\\n",
       "0  monographic  lsidyv35c55757               1         196   \n",
       "1  monographic  lsidyv376da437               1        4047   \n",
       "\n",
       "        fulltext_filename   type   genre  \\\n",
       "0  000000196_01_text.json   poet  Poetry   \n",
       "1  000004047_01_text.json  story   Prose   \n",
       "\n",
       "                                            fulltext  book_id  \n",
       "0        THE POETICAL AVIARY, WITH A B I R D'S-E ...      196  \n",
       "1        HISTORY OF GREECE ABBOTT  A HISTORY OF G...     4047  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide all the data into training and testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode string labels into integers using LabelEncoder from sklearn\n",
    "# The label column in both the training and testing dataframes contains string labels\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit the encoder on the training labels and transform them to integers\n",
    "train_df['label'] = label_encoder.fit_transform(train_df['genre'])\n",
    "# Apply the same transformation on the test labels\n",
    "test_df['label'] = label_encoder.transform(test_df['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for our text data\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.texts = dataframe['fulltext'].values  # Get the 'text' column from the dataframe\n",
    "        self.labels = dataframe['label'].values  # Get the 'label' column (which now contains integers)\n",
    "        self.tokenizer = tokenizer  # BERT tokenizer passed as an argument\n",
    "        self.max_len = max_len  # Maximum length for the tokenized input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)  # Returns the number of samples in the dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]  # Get the text at the given index\n",
    "        label = self.labels[index]  # Get the label corresponding to the text\n",
    "\n",
    "        # Tokenize the text\n",
    "        # encode_plus returns a dictionary with input_ids, attention_mask, etc.\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,  # Add [CLS] and [SEP] tokens\n",
    "            max_length=self.max_len,  # Truncate or pad to the max length\n",
    "            return_token_type_ids=False,  # We don't need token type IDs for classification\n",
    "            padding='longest',  # Pad the sequence to max_len\n",
    "            return_attention_mask=True,  # Return attention mask (which indicates padded tokens)\n",
    "            return_tensors='pt',  # Return PyTorch tensors\n",
    "            truncation=True  # Truncate longer sequences\n",
    "        )\n",
    "\n",
    "        # Return the input ids, attention mask, and label for this sample\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)  # Convert label to tensor\n",
    "        }\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, data_loader, optimizer, loss_fn, device, scheduler, n_examples):\n",
    "    model = model.train()  # Set the model to training mode\n",
    "    total_loss = 0  # Initialize the total loss\n",
    "    correct_predictions = 0  # Initialize correct predictions count\n",
    "\n",
    "    for batch in data_loader:\n",
    "        # Move the data to the device (GPU or CPU)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss  # Get the loss from the output\n",
    "        logits = outputs.logits  # Get the raw output (logits)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Zero out the gradients\n",
    "        loss.backward()  # Perform backpropagation\n",
    "        optimizer.step()  # Update model parameters\n",
    "        scheduler.step()  # Update the learning rate based on the scheduler\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate the total loss\n",
    "        _, preds = torch.max(logits, dim=1)  # Get the predicted labels (the highest logit)\n",
    "        correct_predictions += torch.sum(preds == labels)  # Count correct predictions\n",
    "\n",
    "    return correct_predictions.float() / n_examples, total_loss / len(data_loader)  # Return accuracy and average loss\n",
    "\n",
    "# Function to evaluate the model on the validation set\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0  # Initialize the total loss\n",
    "    correct_predictions = 0  # Initialize correct predictions count\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation (we're not training)\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)  # Move data to the device\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss  # Get the loss\n",
    "            logits = outputs.logits  # Get the raw output (logits)\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate the loss\n",
    "            _, preds = torch.max(logits, dim=1)  # Get the predicted labels\n",
    "            correct_predictions += torch.sum(preds == labels)  # Count correct predictions\n",
    "\n",
    "    return correct_predictions.float() / n_examples, total_loss / len(data_loader)  # Return accuracy and average loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Train loss 1.3777993140013323 accuracy 0.31855955719947815\n",
      "Test loss 1.3249172170956929 accuracy 0.4065934121608734\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train loss 1.321768563726674 accuracy 0.371191143989563\n",
      "Test loss 1.3043472369511921 accuracy 0.450549453496933\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train loss 1.3007759166800457 accuracy 0.40166205167770386\n",
      "Test loss 1.3017238179842632 accuracy 0.4285714328289032\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train loss 1.3041148755861365 accuracy 0.4487534761428833\n",
      "Test loss 1.3015182216962178 accuracy 0.4285714328289032\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train loss 1.3011114027189172 accuracy 0.41274237632751465\n",
      "Test loss 1.3015063405036926 accuracy 0.4285714328289032\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train loss 1.3070047730984895 accuracy 0.42382270097732544\n",
      "Test loss 1.3015062808990479 accuracy 0.4285714328289032\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train loss 1.2946502488592397 accuracy 0.4736842215061188\n",
      "Test loss 1.301506261030833 accuracy 0.4285714328289032\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train loss 1.304159164428711 accuracy 0.4459833800792694\n",
      "Test loss 1.3015063007672627 accuracy 0.4285714328289032\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train loss 1.2943245327990989 accuracy 0.4709141254425049\n",
      "Test loss 1.301506261030833 accuracy 0.4285714328289032\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train loss 1.3058062273523081 accuracy 0.4321329593658447\n",
      "Test loss 1.301506261030833 accuracy 0.4285714328289032\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT tokenizer and model\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'  # Using the uncased version of BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, clean_up_tokenization_spaces=True)  # Load tokenizer\n",
    "\n",
    "# Initialize a data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Hyperparameters\n",
    "FREEZE = False  # Whether to freeze the BERT layers or not\n",
    "MAX_LEN = 512  # Maximum length of input sequences\n",
    "BATCH_SIZE = 16  # Batch size for training and evaluation\n",
    "EPOCHS = 10  # Number of epochs to train the model\n",
    "LEARNING_RATE = 5e-5  # Learning rate for the optimizer\n",
    "\n",
    "# Assume train_df and test_df are the pandas DataFrames containing the dataset\n",
    "# train_df = pd.DataFrame(...)  # DataFrame containing 'fulltext' and 'label'\n",
    "# test_df = pd.DataFrame(...)   # DataFrame containing 'fulltext' and 'label'\n",
    "\n",
    "# Create datasets for training and testing using the TextDataset class\n",
    "train_dataset = TextDataset(train_df, tokenizer, MAX_LEN)\n",
    "test_dataset = TextDataset(test_df, tokenizer, MAX_LEN)\n",
    "\n",
    "# Create DataLoaders to feed data into the model in batches\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=data_collator)  # Shuffle the training data\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=data_collator)  # No need to shuffle the test data\n",
    "\n",
    "# Initialize the pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels=len(label_encoder.classes_))\n",
    "model = model.to(device)  # Move the model to the device (GPU/CPU)\n",
    "\n",
    "# Ensure all layers are trainable\n",
    "# NB this is costly in terms of memory and computation\n",
    "if not FREEZE:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)  # AdamW optimizer (recommended for BERT)\n",
    "total_steps = len(train_data_loader) * EPOCHS  # Total number of training steps\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)  # Learning rate scheduler\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)  # Cross entropy loss (common for classification tasks)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')  # Print the current epoch\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Train the model for one epoch\n",
    "    train_acc, train_loss = train_model(model, train_data_loader, optimizer, loss_fn, device, scheduler, len(train_df))\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')  # Print training loss and accuracy\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    test_acc, test_loss = eval_model(model, test_data_loader, loss_fn, device, len(test_df))\n",
    "    print(f'Test loss {test_loss} accuracy {test_acc}')  # Print validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Drama       0.48      0.46      0.47        24\n",
      "       Music       0.33      0.35      0.34        20\n",
      "      Poetry       0.38      0.31      0.34        26\n",
      "       Prose       0.50      0.62      0.55        21\n",
      "\n",
      "    accuracy                           0.43        91\n",
      "   macro avg       0.42      0.43      0.43        91\n",
      "weighted avg       0.42      0.43      0.42        91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation and metrics (after training is completed)\n",
    "y_true = []  # List to store true labels\n",
    "y_pred = []  # List to store predicted labels\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation (not needed for inference)\n",
    "    for batch in test_data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)  # Forward pass\n",
    "        _, preds = torch.max(outputs.logits, dim=1)  # Get the predicted labels\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())  # Store true labels\n",
    "        y_pred.extend(preds.cpu().numpy())  # Store predicted labels\n",
    "\n",
    "# Print the classification report (precision, recall, F1-score)\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 (Easy): RoBERTa\n",
    "\n",
    "The model clearly could use some improvement. One way it to switch to a better model architecture, which allows larger inputs to be processed. Consider RoBERTa:\n",
    "\n",
    "```Python\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# Change the model name to a RoBERTa pre-trained model\n",
    "PRE_TRAINED_MODEL_NAME = 'roberta-base'  # You can use 'roberta-large' for a larger model\n",
    "\n",
    "# Load the RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "model = RobertaForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(device)\n",
    "```\n",
    "\n",
    "You can remove the line: ```return_token_type_ids=False``` since RoBERTa uses a Byte-pair encoding that does not include type IDs. How is the performance changing? How about the training time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 (Medium): Book type\n",
    "\n",
    "Try using the `type` column instead of genre. Is it evenly distributed or not? How does the classifier perform with it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "In this example, we are given a dataset of toponyms in 19th-century Engish newspapers. We are interested in developing a model to perform Named Entity Recognition, i.e., detecting mentions to entities of interest in a text. [The dataset is accessible here](https://github.com/hipe-eval/HIPE-2022-data/blob/main/documentation/README-topres19th.md#topres19th-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the data looks like:\n",
    "\n",
    "```\n",
    "#FORMAT=WebAnno TSV 3.2\n",
    "#T_SP=webanno.custom.Customentity|identifiier|value\n",
    "\n",
    "#Text=THE POOLE AND SOUTH-WESTERN HERALD, THURSDAY, OCTOBER 20, 1864.\n",
    "1-1\t0-3\tTHE\t_\t_\t\n",
    "1-2\t4-9\tPOOLE\t_\t_\t\n",
    "1-3\t10-13\tAND\t_\t_\t\n",
    "1-4\t14-27\tSOUTH-WESTERN\t_\t_\t\n",
    "1-5\t28-34\tHERALD\t_\t_\t\n",
    "1-6\t34-35\t,\t_\t_\t\n",
    "1-7\t36-44\tTHURSDAY\t_\t_\t\n",
    "1-8\t44-45\t,\t_\t_\t\n",
    "1-9\t46-53\tOCTOBER\t_\t_\t\n",
    "1-10\t54-56\t20\t_\t_\t\n",
    "1-11\t56-57\t,\t_\t_\t\n",
    "1-12\t58-62\t1864\t_\t_\t\n",
    "1-13\t62-63\t.\t_\t_\t\n",
    "\n",
    "#Text=POOLE TOWN COUNCIL.\n",
    "2-1\t65-70\tPOOLE\thttps://en.wikipedia.org/wiki/Poole\tLOC\n",
    "2-2\t71-75\tTOWN\t_\t_\t\n",
    "2-3\t76-83\tCOUNCIL\t_\t_\t\n",
    "2-4\t83-84\t.\t_\t_\t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to load data into memory\n",
    "\n",
    "import os, re, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Path to the dataset directory\n",
    "DATA_DIR = \"data/topRes19th_v2\"\n",
    "\n",
    "allowed_classes = ['LOC','_']\n",
    "# Create the label_to_id dictionary by enumerating the allowed_classes\n",
    "label_to_id = {label: idx for idx, label in enumerate(allowed_classes)}\n",
    "def strip_square_brackets(word):\n",
    "    # Use regex to remove brackets and their contents\n",
    "    return re.sub(r'\\[.*?\\]', '', word)\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data_dir, split=\"train\", context_window=3):\n",
    "        \"\"\"\n",
    "        Initialize the dataset by loading all the files in the specified split (train/test).\n",
    "        Strips out long sequences of non-entity tokens (labeled '_'), and keeps tokens \n",
    "        with 'LOC' labels along with a context window of surrounding tokens.\n",
    "\n",
    "        Args:\n",
    "        - data_dir (str): The directory where the dataset is stored.\n",
    "        - split (str): 'train' or 'test'.\n",
    "        - context_window (int): The number of surrounding tokens to keep around LOC occurrences.\n",
    "        \"\"\"\n",
    "        self.data_dir = os.path.join(data_dir, split, \"annotated_tsv\")\n",
    "        self.files = [f for f in os.listdir(self.data_dir) if f.endswith(\".tsv\")]\n",
    "        self.context_window = context_window  # Context window around LOC tokens\n",
    "        self.data = []\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\" Load and parse the data from the TSV files. \"\"\"\n",
    "        for file in self.files:\n",
    "            file_path = os.path.join(self.data_dir, file)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                tokens, labels = [], []\n",
    "                for line in f:\n",
    "                    if line.strip() == \"\" or line.startswith(\"#\"):  # Skip headers and empty lines\n",
    "                        continue\n",
    "                    columns = line.strip().split(\"\\t\")\n",
    "                    token = columns[2]  # The word/token itself\n",
    "                    label = strip_square_brackets(columns[-1])  # The NER label (LOC, BUILDINGS, etc.)\n",
    "                    if label != \"_\":\n",
    "                        label = \"LOC\"  # Simplify the label to LOC\n",
    "                    tokens.append(token)\n",
    "                    labels.append(label)\n",
    "\n",
    "                # Now we strip the excess '_' tokens and keep the context around 'LOC' tokens\n",
    "                stripped_tokens, stripped_labels = self._strip_non_loc(tokens, labels)\n",
    "\n",
    "                if stripped_tokens:\n",
    "                    self.data.append((stripped_tokens, stripped_labels))\n",
    "\n",
    "    def _strip_non_loc(self, tokens, labels):\n",
    "        \"\"\"\n",
    "        Strips long sequences of tokens labeled as '_', keeping tokens with 'LOC' labels and\n",
    "        a context window of surrounding tokens.\n",
    "\n",
    "        Args:\n",
    "        - tokens (List[str]): List of tokens in a sentence.\n",
    "        - labels (List[str]): List of corresponding labels.\n",
    "\n",
    "        Returns:\n",
    "        - stripped_tokens (List[str]): List of tokens after stripping excess '_'.\n",
    "        - stripped_labels (List[str]): List of labels after stripping excess '_'.\n",
    "        \"\"\"\n",
    "        stripped_tokens = []\n",
    "        stripped_labels = []\n",
    "\n",
    "        loc_indices = [i for i, label in enumerate(labels) if label == \"LOC\"]\n",
    "\n",
    "        if not loc_indices:\n",
    "            # No LOC labels, return empty (this sentence will be dropped)\n",
    "            return stripped_tokens, stripped_labels\n",
    "\n",
    "        # Loop through each LOC token and capture its context\n",
    "        for loc_idx in loc_indices:\n",
    "            # Start and end indices for the context window around LOC\n",
    "            start_idx = max(0, loc_idx - self.context_window)\n",
    "            end_idx = min(len(tokens), loc_idx + self.context_window + 1)\n",
    "\n",
    "            # Append the tokens and labels in the context window\n",
    "            stripped_tokens.extend(tokens[start_idx:end_idx])\n",
    "            stripped_labels.extend(labels[start_idx:end_idx])\n",
    "\n",
    "        return stripped_tokens, stripped_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Returns the total number of articles in the dataset. \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the tokens and labels for the given index.\n",
    "        Args:\n",
    "        - idx (int): The index of the article.\n",
    "        Returns:\n",
    "        - tokens (List[str]): The list of tokens.\n",
    "        - labels (List[str]): The corresponding NER labels for the tokens.\n",
    "        \"\"\"\n",
    "        tokens, labels = self.data[idx]\n",
    "        return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['INDIA', '.', 'Calcutta', ',', 'INDIA', '.', 'Calcutta', ',', 'Aug', '.', 'The', 'Lieutenant-Governor', 'left', 'Calcutta', 'on', 'the', '23rd', '.', 'for', 'the', 'Assam', 'districts', '.', 'The', 'The', 'Bishop', 'of', 'Calcutta', 'is', 'about', 'to', 'Provinces', 'of', 'the', 'Punjaub', '.', 'Bengal', 'is', 'the', 'Punjaub', '.', 'Bengal', 'is', 'reported', 'healthy', 'are', 'good', '.', 'Bombay', ',', 'Aug', '.', 'the', 'Bank', 'of', 'Bombay', 'the', 'directors', 'reported', 'bank', '.', 'The', 'Poonah', 'branch', 'of', 'the', 'the', 'Bank', 'of', 'Bombay', 'expected', 'to', 'lose', 'the', 'Banks', 'of', 'Bombay', 'and', 'Bengal', 'has', 'of', 'Bombay', 'and', 'Bengal', 'has', 'been', 'abandoned', 'latest', 'news', 'from', 'Cabul', 'represents', 'Afzul', 'Khans', 'is', 'expected', 'that', 'Cabul', 'itself', 'will', 'soon']\n",
      "Labels: ['LOC', '_', 'LOC', '_', 'LOC', '_', 'LOC', '_', '_', '_', '_', '_', '_', 'LOC', '_', '_', '_', '_', '_', '_', 'LOC', '_', '_', '_', '_', '_', '_', 'LOC', '_', '_', '_', '_', '_', '_', 'LOC', '_', 'LOC', '_', '_', 'LOC', '_', 'LOC', '_', '_', '_', '_', '_', '_', 'LOC', '_', '_', '_', '_', '_', '_', 'LOC', '_', '_', '_', '_', '_', '_', 'LOC', '_', '_', '_', '_', '_', '_', 'LOC', '_', '_', '_', '_', '_', '_', 'LOC', '_', 'LOC', '_', '_', 'LOC', '_', 'LOC', '_', '_', '_', '_', '_', '_', 'LOC', '_', '_', '_', '_', '_', '_', 'LOC', '_', '_', '_']\n",
      "Batch tokens: [('will',), ('remain',), ('in',), ('England',), (',',), ('to',), ('watch',)]\n",
      "Batch labels: [('_',), ('_',), ('_',), ('LOC',), ('_',), ('_',), ('_',)]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# Load the dataset\n",
    "train_dataset = NERDataset(DATA_DIR, split=\"train\")\n",
    "test_dataset = NERDataset(DATA_DIR, split=\"test\")\n",
    "\n",
    "# Example: Load one article\n",
    "tokens, labels = train_dataset[0]\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch in train_loader:\n",
    "    tokens, labels = batch\n",
    "    print(\"Batch tokens:\", tokens)\n",
    "    print(\"Batch labels:\", labels)\n",
    "    break  # Stop after the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'_': 21848, 'LOC': 18581})\n",
      "Class Weights: tensor([1.0879, 0.9252])\n",
      "{'LOC': 0, '_': 1}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "def calculate_class_weights(train_dataset, label_to_id):\n",
    "    label_counts = Counter()\n",
    "    \n",
    "    for example in train_dataset:\n",
    "        labels = list(map(lambda x: x, example[1]))  # Extract the label from tuple\n",
    "        label_counts.update(labels)\n",
    "    \n",
    "    total_labels = sum(label_counts.values())\n",
    "    class_weights = {}\n",
    "    print(label_counts)\n",
    "    \n",
    "    # Inverse weighting: More frequent classes get lower weights\n",
    "    for label, count in label_counts.items():\n",
    "        class_weights[label_to_id[label]] = total_labels / (len(label_to_id) * count)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    class_weights = torch.tensor([class_weights[i] for i in range(len(label_to_id))], dtype=torch.float)\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "# Example: Calculate class weights\n",
    "class_weights = calculate_class_weights(train_dataset, label_to_id)\n",
    "print(f\"Class Weights: {class_weights}\")\n",
    "print(label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer, label_to_id, max_length=512, label_pad_token=-100):\n",
    "    \"\"\"\n",
    "    Collate function to process batches with tokens and labels stored in tuples.\n",
    "    \n",
    "    Args:\n",
    "    - batch: List of dicts containing 'tokens' and 'labels', where both are tuples.\n",
    "    - tokenizer: Pretrained tokenizer (RobertaTokenizer).\n",
    "    - label_to_id: Dictionary mapping string labels (e.g., 'LOC') to integers.\n",
    "    - max_length: Maximum sequence length (default 512 for RoBERTa).\n",
    "    - label_pad_token: Token to pad labels, default is -100.\n",
    "    \n",
    "    Returns:\n",
    "    - input_ids: Tensor of tokenized input sequences.\n",
    "    - labels: Tensor of padded labels.\n",
    "    - attention_mask: Tensor mask to ignore padding tokens.\n",
    "    \"\"\"\n",
    "    # Extract tokens and labels from tuples\n",
    "    tokens = [list(map(lambda x: x, item[0])) for item in batch]\n",
    "    labels = [list(map(lambda x: x, item[1])) for item in batch]\n",
    "\n",
    "    # Tokenize the tokens with truncation and padding\n",
    "    tokenized_inputs = tokenizer(tokens, \n",
    "                                 is_split_into_words=True, \n",
    "                                 padding=True, \n",
    "                                 truncation=True, \n",
    "                                 max_length=max_length,  # Ensure max length, also drops what is longer!\n",
    "                                 return_tensors=\"pt\")\n",
    "    \n",
    "    # Map string labels to integers and pad them to match input length\n",
    "    max_len = tokenized_inputs['input_ids'].shape[1]  # Get max length from tokenized input\n",
    "    padded_labels = []\n",
    "    for label_seq in labels:\n",
    "        # Convert string labels to IDs\n",
    "        label_ids = [label_to_id.get(label, label_pad_token) for label in label_seq]\n",
    "        padded_label = label_ids[:max_len] + [label_pad_token] * (max_len - len(label_ids))  # Truncate and pad\n",
    "        padded_labels.append(padded_label)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    padded_labels = torch.tensor(padded_labels, dtype=torch.long)\n",
    "    \n",
    "    return tokenized_inputs['input_ids'], padded_labels, tokenized_inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6667300462722778\n",
      "Epoch 2, Loss: 0.6363546848297119\n",
      "Epoch 3, Loss: 0.6143303513526917\n",
      "Epoch 4, Loss: 0.6364577412605286\n",
      "Epoch 5, Loss: 0.6191195249557495\n",
      "Epoch 6, Loss: 0.6238633394241333\n",
      "Epoch 7, Loss: 0.5848383903503418\n",
      "Epoch 8, Loss: 0.5780908465385437\n",
      "Epoch 9, Loss: 0.5862990617752075\n",
      "Epoch 10, Loss: 0.558539628982544\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import RobertaTokenizer, RobertaForTokenClassification\n",
    "\n",
    "# Load pre-trained RoBERTa model and tokenizer\n",
    "model = RobertaForTokenClassification.from_pretrained('roberta-base', num_labels=len(allowed_classes))\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', clean_up_tokenization_spaces=True)\n",
    "\n",
    "BATCH_SIZE = 50  # Adjust batch size as necessary\n",
    "EPOCHS = 10  # Adjust epochs as necessary\n",
    "\n",
    "# Create PyTorch datasets and loaders\n",
    "train_dataset = NERDataset(DATA_DIR, split=\"train\")\n",
    "test_dataset = NERDataset(DATA_DIR, split=\"test\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=lambda batch: collate_fn(batch, tokenizer, label_to_id))\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=lambda batch: collate_fn(batch, tokenizer, label_to_id))\n",
    "\n",
    "model.to(device)  # Move model to device\n",
    "\n",
    "# Define the weighted loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(device), ignore_index=-100)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Ensure all layers are trainable\n",
    "# NB this is costly in terms of memory and computation\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Training loop with device placement\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):  # Adjust epochs as necessary\n",
    "    for tokens, labels, mask in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move data to device\n",
    "        tokens = tokens.to(device)\n",
    "        labels = labels.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=tokens, attention_mask=mask)\n",
    "        \n",
    "        # Compute loss with weighted CrossEntropyLoss\n",
    "        loss = loss_fn(outputs.logits.view(-1, len(label_to_id)), labels.view(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1497 1912]\n",
      " [1815 5354]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.45      0.44      0.45      3409\n",
      "           _       0.74      0.75      0.74      7169\n",
      "\n",
      "    accuracy                           0.65     10578\n",
      "   macro avg       0.59      0.59      0.59     10578\n",
      "weighted avg       0.65      0.65      0.65     10578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Prepare lists to accumulate predictions and true labels across batches\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Evaluation on the test set with device placement\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for tokens, labels, mask in test_loader:\n",
    "        # Move data to device\n",
    "        tokens = tokens.to(device)\n",
    "        labels = labels.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        outputs = model(input_ids=tokens, attention_mask=mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "        # Convert predictions and labels back to CPU and flatten the arrays\n",
    "        all_preds.append(predictions.cpu().numpy().flatten())\n",
    "        all_labels.append(labels.cpu().numpy().flatten())\n",
    "\n",
    "# Concatenate all batches into a single array\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "# Filter out padding tokens (ignore -100 in labels)\n",
    "mask = all_labels != -100\n",
    "all_preds = all_preds[mask]\n",
    "all_labels = all_labels[mask]\n",
    "\n",
    "# Get the class names (in the same order as `label_to_id`)\n",
    "class_names = [x[0] for x in sorted(label_to_id.items(), key=lambda item: item[1])]\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report with micro and macro averages\n",
    "report = classification_report(all_labels, all_preds, target_names=class_names, zero_division=0)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 (Easy)\n",
    "\n",
    "The results are not outstanding, but there is a lot we can tweak and try out to make them better. Try to change pretrained model, configure hyperparameters differently, and see if you can improve upon results.\n",
    "\n",
    "This model is intense to train. If you experience excessive delays, attempt to reduce the data you use to train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 (Easy)\n",
    "\n",
    "Experiment with the context window parameter. What happens when you reduce it or when you increase it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 (Medium)\n",
    "\n",
    "The original dataset contains more classes than `LOC`. Check them and see how frequent they are. Second, keep them into the dataset and train the model again using multiple classes. How does the model performance changes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
