{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Vision\n",
    "\n",
    "In this notebook, we explore architectures for machine vision, i.e., when the inputs are images. While we will work with image classification, several tasks fall within the remits of machine vision. Examples include object segmentation and automatic text recognition.\n",
    "\n",
    "The references for this part are:\n",
    "* [Computer Vision for the Humanities: An Introduction to Deep Learning for Image Classification](https://programminghistorian.org/en/lessons/computer-vision-deep-learning-pt1) from the Programming Historian series.\n",
    "* [Tutorial 5: Inception, ResNet, DenseNet](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial5/Inception_ResNet_DenseNet.html) from the UvA DL course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us remember the usual pipeline for Machine Learning tasks, as follows:\n",
    "\n",
    "<img src=\"figures/en-or-computer-vision-deep-learning-pt1-06.png\" width=\"900px\" heigth=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import os, random\n",
    "import pandas as pd\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "sns.set_theme('notebook', style='whitegrid')\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42) # Setting the seed\n",
    "\n",
    "print(\"Using torch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "\n",
    "The dataset from the Programming Historian lesson is [available here](https://github.com/davanstrien/Programming-Historian-Computer-Vision-Lessons-submission/tree/main), please download it locally to run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Create a mapping from string labels to integer labels\n",
    "        self.label_to_index = {label: idx for idx, label in enumerate(self.annotations['label'].unique())}\n",
    "        # Create a reverse mapping for integer labels back to string labels\n",
    "        self.index_to_label = {idx: label for label, idx in self.label_to_index.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label_str = self.annotations.iloc[index, 1]\n",
    "        # Convert string label to integer\n",
    "        label = self.label_to_index[label_str]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Paths to the data\n",
    "csv_file = './data/newspaper_images/ads_data/ads_upsampled_no_index.csv'  # Path to your CSV file\n",
    "img_dir = './data/newspaper_images/ads_data/images'       # Directory with all the images\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the images to 224x224 without preserving aspect ratio, i.e., squishing the image\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # This is the mean and std deviation of the ImageNet dataset, in view of using a pre-trained ResNet model\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = CustomImageDataset(csv_file=csv_file, img_dir=img_dir, transform=transform)\n",
    "\n",
    "# Define the train-validation split\n",
    "train_size = int(0.8 * len(dataset))  # 80% of the data for training\n",
    "val_size = len(dataset) - train_size  # Remaining 20% for validation\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for train and validation datasets\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Example: Iterate through the training data\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Train - images shape: {images.shape}, labels shape: {labels.shape}\")\n",
    "    break\n",
    "\n",
    "# Example: Iterate through the validation data\n",
    "for images, labels in val_loader:\n",
    "    print(f\"Validation - images shape: {images.shape}, labels shape: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plot one image with its label\n",
    "def plot_example_image(dataset, index):\n",
    "    image, label = dataset[index]\n",
    "    \n",
    "    # Unnormalize the image (this should be done before converting to NumPy)\n",
    "    image = image * torch.tensor([0.229, 0.224, 0.225]).unsqueeze(1).unsqueeze(2) + torch.tensor([0.485, 0.456, 0.406]).unsqueeze(1).unsqueeze(2)\n",
    "    \n",
    "    # Convert the image tensor to a NumPy array for plotting\n",
    "    image = image.permute(1, 2, 0).numpy()  # Change from CxHxW to HxWxC\n",
    "    image = image.clip(0, 1)  # Clip values to ensure they are between 0 and 1\n",
    "\n",
    "    # Get the label string from the index\n",
    "    label_str = dataset.index_to_label[label]\n",
    "\n",
    "    # Plotting the image with its label\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Label: {label_str}\")\n",
    "    plt.axis('off')  # Turn off the axis\n",
    "    plt.show()\n",
    "\n",
    "# Plot an example image from the dataset (e.g., the first image)\n",
    "plot_example_image(dataset, index=79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use below a well-established architecture for machine vision tasks, called **ResNet18**. This is the smallest ResNet available. [The ResNet architecture was introduced in Kaiming He et al. in 2015](https://arxiv.org/abs/1512.03385).\n",
    "\n",
    "Before discussing ResNet, we need to explain **Convolutional Neural Networks (CNNs)**, the building block.\n",
    "\n",
    "## Understanding Convolutional Neural Networks (CNNs)\n",
    "\n",
    "**Convolutional Neural Networks (CNNs)** are a class of deep learning models specifically designed to process and analyze data that has a grid-like topology, such as images. CNNs are a type of artificial neural network and are particularly well-suited for tasks like image recognition, object detection, and various other computer vision tasks.\n",
    "\n",
    "#### Key Components of CNNs:\n",
    "\n",
    "1. **Convolutional Layers**:\n",
    "   - The core building block of a CNN is the convolutional layer, which applies a set of filters (also known as kernels) across the input image. These filters move across the image spatially (in two dimensions), performing element-wise multiplications and summing up the results. This operation is called a convolution.\n",
    "   - The filters help detect features such as edges, textures, and patterns in the image. As you move deeper into the network, the filters become more complex, detecting higher-level features like shapes and objects.\n",
    "\n",
    "<img src=\"figures/Convolutional_neural_network.png\" width=\"600px\" heigth=\"600px\">\n",
    "\n",
    "*[From Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network)*.\n",
    "\n",
    "2. **Pooling Layers**:\n",
    "   - Pooling layers are used to reduce the spatial dimensions of the feature maps, making the computation more efficient and helping to achieve some level of spatial invariance. The most common pooling operation is max pooling, which selects the maximum value in each patch of the feature map.\n",
    "   - Pooling helps in reducing the number of parameters and computations in the network, which can help prevent overfitting.\n",
    "\n",
    "<img src=\"figures/Max_pooling.png\" width=\"600px\" heigth=\"600px\">\n",
    "\n",
    "*[From Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network)*.\n",
    "\n",
    "3. **Fully Connected Layers**:\n",
    "   - After a series of convolutional and pooling layers, the output is typically passed through one or more fully connected layers (similar to a traditional neural network). These layers combine the features extracted by the convolutional layers to classify the input image or perform other tasks like regression.\n",
    "\n",
    "4. **Activation Functions**:\n",
    "   - CNNs use activation functions like ReLU (Rectified Linear Unit) to introduce non-linearity into the model. This non-linearity helps the network learn more complex patterns.\n",
    "\n",
    "5. **Dropout Layers**:\n",
    "   - To prevent overfitting, dropout layers are often included, which randomly deactivate a portion of the neurons during training. This encourages the network to learn more robust features.\n",
    "\n",
    "#### How CNNs Work:\n",
    "When an image is passed through a CNN, each layer applies a series of transformations to the image, gradually abstracting and recognizing more complex features. For example, the first layers might detect simple edges, while deeper layers might detect parts of objects like eyes, and the final layers might recognize full objects like faces.\n",
    "\n",
    "#### Advantages of CNNs:\n",
    "- **Spatial Hierarchy**: CNNs leverage the hierarchical structure of images, where lower layers capture basic features and higher layers capture more complex patterns.\n",
    "- **Parameter Efficiency**: By sharing the weights of filters across the image, CNNs require fewer parameters than fully connected networks, making them more efficient and less prone to overfitting.\n",
    "\n",
    " [A visual CNN explained can be found here](https://poloclub.github.io/cnn-explainer/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Architectures\n",
    "\n",
    "**ResNet** (Residual Network) is a family of deep convolutional neural networks introduced by [Kaiming He et al. in 2015](https://arxiv.org/abs/1512.03385). The architecture is designed to tackle the challenge of training very deep networks, which typically suffer from problems like vanishing gradients and degradation. ResNet overcomes these issues using residual learning, where shortcut connections bypass one or more layers. \n",
    "\n",
    "Here is an illustration of the ResNet (right side), compared with previous architectures:\n",
    "\n",
    "<img src=\"figures/res_net.png\">\n",
    "\n",
    "#### ResNet-18\n",
    "- **Depth**: 18 layers.\n",
    "- **Architecture**: ResNet-18 uses basic residual blocks, each containing two 3x3 convolutional layers followed by a ReLU activation function. The residual connections bypass these two layers and add the input directly to the output. This network is relatively shallow and is computationally efficient, making it suitable for smaller tasks and scenarios where resources are constrained.\n",
    "- **Use Case**: Suitable for tasks requiring moderate complexity, often used in academic settings for benchmarks and on datasets that do not require deep architectures.\n",
    "\n",
    "#### ResNet-50\n",
    "- **Depth**: Depth: 50 layers\n",
    "- **Architecture**: ResNet-50 introduces a more complex block called the bottleneck block. A bottleneck block consists of three layers: a 1x1 convolutional layer for reducing dimensionality, a 3x3 convolutional layer, and another 1x1 convolutional layer to restore the dimensionality. This block reduces the number of parameters while maintaining the networkâ€™s depth.\n",
    "- **Use Case**: Often used in practical applications where a balance between model complexity and performance is needed. ResNet-50 is more computationally intensive than ResNet-18 and ResNet-34 but provides better performance on complex tasks.\n",
    "\n",
    "There are several other ResNet architectures available, also via PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, training, and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet model\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Freeze the layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the last layer to a new one, and match the number of classes\n",
    "num_features = model.fc.in_features\n",
    "num_classes = len(dataset.label_to_index)\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'models/resnet_model.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single batch from the validation loader\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "data_iter = iter(val_loader)\n",
    "images, true_labels = next(data_iter)\n",
    "\n",
    "# Move data to the device\n",
    "images = images.to(device)\n",
    "true_labels = true_labels.to(device)\n",
    "\n",
    "# Perform a forward pass to get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    probabilities = F.softmax(outputs, dim=1)\n",
    "\n",
    "# Get the predicted label and confidence\n",
    "_, predicted_labels = torch.max(outputs, 1)\n",
    "confidence, _ = torch.max(probabilities, 1)\n",
    "\n",
    "# Randomly select an index for the image to display\n",
    "index = random.randint(0, images.size(0) - 1)\n",
    "image = images[index].cpu().permute(1, 2, 0).numpy()  # Convert to HxWxC format for plotting\n",
    "true_label = true_labels[index].item()\n",
    "predicted_label = predicted_labels[index].item()\n",
    "confidence_score = confidence[index].item()\n",
    "\n",
    "# Convert the label indexes back to string labels using the dataset's index_to_label dictionary\n",
    "true_label_str = val_dataset.dataset.index_to_label[true_label]\n",
    "predicted_label_str = val_dataset.dataset.index_to_label[predicted_label]\n",
    "\n",
    "# Print the true label, predicted label, and confidence\n",
    "print(f\"True Label: {true_label_str}\")\n",
    "print(f\"Predicted Label: {predicted_label_str}\")\n",
    "print(f\"Model Confidence: {confidence_score:.4f}\")\n",
    "\n",
    "# Optionally, display the image\n",
    "plt.imshow(image)\n",
    "plt.title(f\"True: {true_label_str}, Predicted: {predicted_label_str}, Confidence: {confidence_score:.4f}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 (Easy)\n",
    "\n",
    "Modify the code to use different pre-trained architectures. [See here for ideas.](https://pytorch.org/vision/stable/models.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 (Medium)\n",
    "\n",
    "Implement a simple convolutional neural network to perform binary image classification. [Check this tutorial for inspiration.](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 (Hard)\n",
    "\n",
    "Look at the [WikiArt dataset](https://github.com/cs-chan/ArtGAN/blob/master/WikiArt%20Dataset/README.md). Pick a task of your choice among artist, genre, and style classification. Design, implement, and evaluate a model for this task. Consider starting by reusing the simple model we devised above, and expand from it using more advanced architectures. You can use pre-trained models or implement your own approach from scratch, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
