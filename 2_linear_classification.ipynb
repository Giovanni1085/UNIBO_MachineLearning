{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classification\n",
    "\n",
    "Topics:\n",
    "1. Linear classification\n",
    "2. The Sigmoid function and the Softmax\n",
    "3. Stochastic Gradient Descent solution\n",
    "4. Evaluation\n",
    "5. Implementations\n",
    "6. Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os, codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Regression**: predict real/continuous values given inputs (e.g., a salary of 2.15 ducats a year).\n",
    "* **Classification**: predict categorical values given inputs (e.g., there is a female guarantor in the contract or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid function\n",
    "\n",
    "This is used for binary classification, the Softmax generalizes it to multiple classes.\n",
    "\n",
    "The Sigmoid function takes a line and \"squeezes\" it into a shape more suitable for binary decisions.\n",
    "\n",
    "Model:\n",
    "\n",
    "$\\hat{y} = \\frac{1}{1 + e^{-\\big(b + \\sum_{j=1}^{d}w_j x_j \\big)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of a sigmoid function\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-10,10,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([sigmoid(x) for x in np.linspace(-10,10,1000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "Model, in matrix form (4.1.5):\n",
    "\n",
    "$O = XW + b$\n",
    "\n",
    "$\\hat{Y} = softmax(O)$\n",
    "\n",
    "Where:\n",
    "\n",
    "$softmax(o) = \\frac{exp(o)}{\\sum_{j=1}^q exp(o_j)}$\n",
    "\n",
    "We call $\\textbf{o}$ the logits, and $q$ is the number of classes. The loss function we use is the well-known cross-entropy loss (4.1.8):\n",
    "\n",
    "$l(y,\\hat{y}) = - \\sum_{j=1}^q y_j log(\\hat{y}_j)$\n",
    "\n",
    "The derivative of the loss, w.r.t. any logit $o_i$ is:\n",
    "\n",
    "$\\partial_{o_i} l(y,\\hat{y}) = softmax(o_i) - y_i$\n",
    "\n",
    "And the derivative w.r.t. weights is, by chain rule:\n",
    "\n",
    "$\\partial_{W} L(Y,\\hat{Y}) = X^T * (softmax(O) - Y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock training data\n",
    "X_train = np.array([[0.1, 0.5, 0.4],\n",
    "                    [0.3, 0.2, 0.5],\n",
    "                    [0.6, 0.1, 0.3],\n",
    "                    [0.8, 0.1, 0.1]])\n",
    "\n",
    "# Note that we use one-hot encoding\n",
    "y_train = np.array([[1, 0, 0],\n",
    "                    [0, 1, 0],\n",
    "                    [0, 0, 1],\n",
    "                    [1, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    # We make all logits below zero to avoid overflow errors, see 4.5.2\n",
    "    exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    # Assuming y_true is one-hot encoded\n",
    "    # We add a little number to the predictions to avoid underflow errors, see 4.5.2\n",
    "    m = y_true.shape[0]\n",
    "    return (1/m) * -np.sum(y_true * np.log(y_pred + 1e-12))\n",
    "\n",
    "def compute_gradients(X, y_true, y_pred):\n",
    "    # Compute the gradient of the cross-entropy loss with respect to the logits\n",
    "    m = y_true.shape[0]\n",
    "    grad = (1/m) * np.matmul(X.T,(y_pred - y_true))\n",
    "    return grad\n",
    "\n",
    "def update_weights(weights, gradients, learning_rate):\n",
    "    return weights - learning_rate * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights\n",
    "np.random.seed(42)\n",
    "weights = np.random.rand(X_train.shape[1], y_train.shape[1])\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logits = np.matmul(X_train, weights)\n",
    "    y_pred = softmax(logits)\n",
    "    \n",
    "    loss = cross_entropy_loss(y_train, y_pred)\n",
    "    gradients = compute_gradients(X_train, y_train, y_pred)\n",
    "    weights = update_weights(weights, gradients, learning_rate)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "\n",
    "# Display final weights\n",
    "print(\"Final weights:\", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "\n",
    "X_test = np.array([[0.3, 0.3, 0.4],\n",
    "                    [0.9, 12, 0.8]])\n",
    "y_pred = softmax(np.matmul(X_test,weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(y_pred,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_pred,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do another example with more data\n",
    "\n",
    "from sklearn.datasets import make_classification # a simple way to create classification data\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=0, n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make y to one hot encoding\n",
    "y_one_hot = np.zeros((y_train.size, y_train.max() + 1))\n",
    "y_one_hot[np.arange(y_train.size), y_train] = 1\n",
    "y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights\n",
    "np.random.seed(42)\n",
    "weights = np.random.rand(X_train.shape[1], y_one_hot.shape[1])\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logits = np.dot(X_train, weights)\n",
    "    y_pred = softmax(logits)\n",
    "    \n",
    "    loss = cross_entropy_loss(y_one_hot, y_pred)\n",
    "    gradients = compute_gradients(X_train, y_one_hot, y_pred)\n",
    "    weights = update_weights(weights, gradients, learning_rate)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "\n",
    "# Display final weights\n",
    "print(\"Final weights:\", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We can use the loss, but this is not very informative. More often, in classification, we use the **accuracy**: the fraction of correctly labelled datapoints in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = softmax(np.matmul(X_test,weights))\n",
    "y_pred = np.ravel(np.argmax(y_pred,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations\n",
    "\n",
    "Ad usual **sklearn** is a great starting point. See the [LogisticRegression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) (despite the name, it does classification..)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some data as before\n",
    "\n",
    "from sklearn.datasets import make_classification # a simple way to create classification data\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=0, n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html#sphx-glr-auto-examples-linear-model-plot-iris-logistic-py\n",
    "# Let's train a model, make predictions and show the decision boundary\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg.fit(X, y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "h = .01  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.2)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.RdYlBu)\n",
    "plt.xlabel('Sepal length', fontsize=14)\n",
    "plt.ylabel('Sepal width', fontsize=14)\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "* Extend to multiple classes, and investigate how LogisticRegression allows you to integrate regularization.\n",
    "* Implement the same classification task using [an MLP](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#multi-layer-perceptron)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Tesla vs SpaceX tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "\n",
    "root_folder = \"data/musk_tweets\"\n",
    "df_elon = pd.read_csv(codecs.open(os.path.join(root_folder,\"df_elon.csv\"), encoding=\"utf8\"), sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elon.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to just Tesla or SpaceX\n",
    "\n",
    "def classify_t_or_s(tweet):\n",
    "    \n",
    "    if type(tweet) == str and \"@SpaceX\" in tweet:\n",
    "        return 1\n",
    "    elif type(tweet) == str and \"@TeslaMotors\" in tweet:\n",
    "        return 0\n",
    "    return -1\n",
    "\n",
    "def remove_t_or_s(tweet):\n",
    "    \n",
    "    if type(tweet) == str:\n",
    "        removed = tweet.replace(\"@SpaceX\",\"\")\n",
    "        removed = removed.replace(\"@TeslaMotors\",\"\")\n",
    "        return removed\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classes and remove the discriminant token!\n",
    "df_elon[\"class\"] = df_elon[\"clean_text\"].apply(classify_t_or_s)\n",
    "df_elon[\"clean_text_rm\"] = df_elon[\"clean_text\"].apply(remove_t_or_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elon[df_elon[\"class\"] == 1].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = df_elon[df_elon[\"class\"] != -1]\n",
    "df_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We tokenize the dataset using NLTK, and create the TF-IDF representation using Sklearn\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(preserve_case=False, reduce_len=False, strip_handles=False)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(tokenizer=tknzr.tokenize)\n",
    "X = count_vect.fit_transform(df_reduced.clean_text_rm)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_reduced[\"class\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it out\n",
    "\n",
    "test_tweet1 = \"Let's all go to Mars and colonize!\"\n",
    "test_tweet2 = \"Let's all go on with an electric car with auto pilot\"\n",
    "x = count_vect.transform([test_tweet1])\n",
    "print(logreg.predict(x)[0])\n",
    "print(logreg.predict_proba(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*But, does it actually work?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory vs confirmatory modelling\n",
    "\n",
    "\"Traditionally, the focus of modelling is on *inference, or for confirming that an hypothesis is \"true\"*. Doing this correctly is not complicated, but it is hard. There is a pair of ideas that you must understand in order to do inference correctly:\n",
    "\n",
    "* Each observation can either be used for exploration or confirmation, not both.\n",
    "\n",
    "* You can use an observation as many times as you like for exploration, but you can only use it once for confirmation. As soon as you use an observation twice, you’ve switched from confirmation to exploration.\n",
    "\n",
    "This is necessary because to confirm a hypothesis you must use data independent of the data that you used to generate the hypothesis. Otherwise you will be over optimistic. There is absolutely nothing wrong with exploration, but you should never sell an exploratory analysis as a confirmatory analysis because it is fundamentally misleading.\n",
    "\n",
    "If you are serious about doing a confirmatory analysis, one approach is to split your data into three pieces before you begin the analysis:\n",
    "\n",
    "* 60% of your data goes into a **training** (or exploration) set. You’re allowed to do anything you like with this data: visualise it and fit tons of models to it.\n",
    "\n",
    "* 20% goes into a **validation** set. You can use this data to compare models or visualisations by hand, but you’re not allowed to use it as part of an automated process.\n",
    "\n",
    "* 20% is held back for a **test** set. You can only use this data ONCE, to test your final model.\n",
    "\n",
    "This partitioning allows you to explore the training data, occasionally generating candidate hypotheses that you check with the query set. When you are confident you have the right model, you can check it once with the test data.\"\n",
    "\n",
    "https://r4ds.had.co.nz/model-intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_hat_test))\n",
    "print(accuracy_score(y_test, y_hat_test, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are doing well already! Try to play with the model or with the pre-processing and test against the test set. When you are happy, do a final evaluation against the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "* Read about precision and recall: https://en.wikipedia.org/wiki/Precision_and_recall. Add them to the evaluation above. Hint: see the `precision_score` and `recall_score` here: https://scikit-learn.org/stable/modules/model_evaluation.html.\n",
    "* Build a multiclass logistic classifier for the BL book genre (perhaps during the afternoon session)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
